# -*- coding: utf-8 -*-
"""transformer_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YoCodx8LS1qFmzB2wBk4UqKwwYskgXvc
"""

# Uninstall conflicting versions
!pip uninstall -y torch torchvision torchaudio

# Install compatible versions of torch, torchvision, and torchaudio
!pip install torch torchvision torchaudio
!pip install transformers
!pip install accelerate
!pip install datasets

# Import libraries
from google.colab import files
import pandas as pd
from transformers import BertTokenizer
import torch
from transformers import DistilBertForMaskedLM, Trainer, TrainingArguments
from datasets import Dataset
import numpy as np

# Upload the CSV file
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

# Read the CSV file into a DataFrame
df = pd.read_csv(file_name)

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the URLs
urls = df['URL'].tolist()

# Prepare the dataset for BERT fine-tuning
def tokenize_function(url):
    return tokenizer(url, truncation=True, padding='max_length', max_length=512)

# Apply the tokenizer to the URLs
tokenized_urls = [tokenize_function(url) for url in urls]

# Convert to a format suitable for Hugging Face's Dataset
input_ids = [url['input_ids'] for url in tokenized_urls]
attention_masks = [url['attention_mask'] for url in tokenized_urls]

# Create labels for masked language modeling
labels = np.copy(input_ids)
# Randomly mask some tokens for training
rand = np.random.rand(*labels.shape)
mask_arr = (rand < 0.15) * (labels != tokenizer.pad_token_id) * (labels != tokenizer.cls_token_id) * (labels != tokenizer.sep_token_id)
selection = np.where(mask_arr == True)

labels[selection] = tokenizer.mask_token_id

# Create a Dataset object
dataset = Dataset.from_dict({'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': labels.tolist()})

# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=False,
)

# Load the pretrained model
model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained('./fine-tuned-distilbert-url')
tokenizer.save_pretrained('./fine-tuned-distilbert-url')